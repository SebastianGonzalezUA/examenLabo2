{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trabajo práctico de Laboratorio de Datos II, Universidad Austral\n## Profesores: Rafael Crescenzi y Pablo Albani\n## Alumno: Sebastián Nicolás González","metadata":{}},{"cell_type":"markdown","source":"### Librerías ------------------------------","metadata":{}},{"cell_type":"code","source":"## Librerías básicas\nimport pandas as pd \nimport numpy as np\nimport json\nimport time\nimport random\n\n## Librerías sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV,train_test_split\n\n## Librerías de algoritmos de aprendizaje\nimport lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Construcción del dataset -------------------------\n","metadata":{}},{"cell_type":"code","source":"## Importo los datos de entrenamiento y testeo\n\ntrain = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv').set_index(\"PetID\")\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv').set_index(\"PetID\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importo los embedding de las imágenes\n## Link a la notebook: https://www.kaggle.com/code/gonzalezsn/ex-menlaboratorio2-imageprocessing-sng\n\npicTrain = pd.read_parquet(\"/kaggle/input/img-sg/train_img_features.parquet\")\npicTest = pd.read_parquet(\"/kaggle/input/img-sg/test_img_features.parquet\")\n\npicTrain.columns = ['pic_'+str(i) for i in range(0,len(picTrain.columns))]\npicTest.columns = ['pic_'+str(i) for i in range(0,len(picTest.columns))]\n\n## Agrego embeddings a train y test \n\ntrain = train.join(picTrain)\ntest = test.join(picTest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importo los embedding de las descripciones\n## Comentario: Probé usando el embedding generado a partir de SBERT, pero hizo que se desplomara el Score de la predicción. Entonces uso el generado a partir de LSA (usando SVD).\n## ## Link a la notebook: https://www.kaggle.com/code/gonzalezsn/ex-menlaboratorio2-textprocessing-sng\n\ntextTrain = pd.read_parquet(\"/kaggle/input/text-sg/train_text_features.parquet\")\ntextTest = pd.read_parquet(\"/kaggle/input/text-sg/test_text_features.parquet\")\n\n## Anexo a train y test\ntrain = pd.concat((train, textTrain), axis=1)\ntest = pd.concat((test, textTest), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Feature Engineering sobre Train ------------------------------------------------------------------------------------------------\n\n# Variable nominal que indica si la mascota tiene o no mas de  un año.\ntrain[\"Age_cat\"] = train[\"Age\"].apply(lambda x: x if x<=12 else 99)\n\n# Variable nominal que indica si fue adoptado de forma gratuita.\ntrain['Adopt_Free'] = np.where(train['Fee'] == 0, 1, 0)\n\n# Variable nominal indicando si tiene las palabras \"adopted\" o \"adoption\".\ntrain['Description'].fillna(\"missing\", inplace=True)\ntrain['DescContain_Adop'] = train['Description'].apply(lambda x: 1 if \"adopted\" in x.lower() else 2 if 'adoption' in x.lower() else 0)\n\n# Variable nominal que indica si el rescatador del animal tiene mas de un animal rescatado.\nresc_count = train.reset_index()[[\"RescuerID\",\"PetID\"]].groupby(by='RescuerID').agg({'PetID': lambda x: (x.count() > 1).astype(np.int8)})\nresc_count.rename(columns={'PetID': 'CountRescMasDe1'}, inplace=True)\ntrain = train.join(resc_count, how='left', on='RescuerID')\n\n# resc_count = train.reset_index()[[\"RescuerID\",\"PetID\"]].groupby(by=\"RescuerID\").agg({\"PetID\":\"count\"})\n# resc_count.rename(columns={'PetID': 'CountResc'}, inplace=True)\n# train = train.join(resc_count, how='left', on='RescuerID')\n\n# Otro FE\ntrain['name_length'] = train['Name'].apply(lambda x: len(x) if type(x) is str else 0)  # Largo del nombre\ntrain['desc_length'] = train['Description'].apply(lambda x: len(x) if type(x) is str else 0)  # Largo de la descripción\ntrain['purebred'] = train['Breed2'].apply(lambda x: 0 if x == 0 else 1)  # Si es raza pura\ntrain['photo_and_video'] = ((train['PhotoAmt'] > 0) & (train['VideoAmt'] > 0)).astype(int)  # Si tiene fotos Y videos\ntrain['photo_or_video'] = ((train['PhotoAmt'] > 0) | (train['VideoAmt'] > 0)).astype(int)  # Si tiene fotos O videos\n\n\n#### Feature Engineering sobre Test ------------------------------------------------------------------------------------------------\n\n# Variable nominal que indica si la mascota tiene o no mas de  un año.\ntest[\"Age_cat\"] = test[\"Age\"].apply(lambda x: x if x<=12 else 99)\n\n# Variable nominal que indica si fue adoptado de forma gratuita.\ntest['Adopt_Free'] = np.where(test['Fee'] == 0, 1, 0)\n\n# Creo una variable nominal indicando si tiene las palabras \"adopted\" o \"adoption\".\ntest['Description'].fillna(\"missing\", inplace=True)\ntest['DescContain_Adop'] = test['Description'].apply(lambda x: 1 if \"adopted\" in x.lower() else 2 if 'adoption' in x.lower() else 0)\n\n# Variable nominal que indica si el rescatador del animal tiene mas de un animal rescatado.\nresc_count = test.reset_index()[[\"RescuerID\",\"PetID\"]].groupby(by='RescuerID').agg({'PetID': lambda x: (x.count() > 1).astype(np.int8)})\nresc_count.rename(columns={'PetID': 'CountRescMasDe1'}, inplace=True)\ntest = test.join(resc_count, how='left', on='RescuerID')\n\n# resc_count = test.reset_index()[[\"RescuerID\",\"PetID\"]].groupby(by=\"RescuerID\").agg({\"PetID\":\"count\"})\n# resc_count.rename(columns={'PetID': 'CountResc'}, inplace=True)\n# test = test.join(resc_count, how='left', on='RescuerID')\n\n# Otro FE\ntest['name_length'] = test['Name'].apply(lambda x: len(x) if type(x) is str else 0)  # Largo del nombre\ntest['desc_length'] = test['Description'].apply(lambda x: len(x) if type(x) is str else 0)  # Largo de la descripción\ntest['purebred'] = test['Breed2'].apply(lambda x: 0 if x == 0 else 1)  # Si es raza pura\ntest['photo_and_video'] = ((test['PhotoAmt'] > 0) & (test['VideoAmt'] > 0)).astype(int)  # Si tiene fotos Y videos\ntest['photo_or_video'] = ((test['PhotoAmt'] > 0) | (test['VideoAmt'] > 0)).astype(int)  # Si tiene fotos O videos","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importo el resultado del análisis de sentimientos (provisto por la competencia)\n\ntrain_id = train.index\ntest_id = test.index\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('/kaggle/input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntrain.loc[:, 'doc_sent_score'] = doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('/kaggle/input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntest.loc[:, 'doc_sent_score'] = doc_sent_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importo la metadata (provisto por la competencia)\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('/kaggle/input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrain.loc[:, 'vertex_x'] = vertex_xs\ntrain.loc[:, 'vertex_y'] = vertex_ys\ntrain.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain.loc[:, 'dominant_blue'] = dominant_blues\ntrain.loc[:, 'dominant_green'] = dominant_greens\ntrain.loc[:, 'dominant_red'] = dominant_reds\ntrain.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain.loc[:, 'dominant_score'] = dominant_scores\ntrain.loc[:, 'label_description'] = label_descriptions\ntrain.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('/kaggle/input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntest.loc[:, 'vertex_x'] = vertex_xs\ntest.loc[:, 'vertex_y'] = vertex_ys\ntest.loc[:, 'bounding_confidence'] = bounding_confidences\ntest.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest.loc[:, 'dominant_blue'] = dominant_blues\ntest.loc[:, 'dominant_green'] = dominant_greens\ntest.loc[:, 'dominant_red'] = dominant_reds\ntest.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest.loc[:, 'dominant_score'] = dominant_scores\ntest.loc[:, 'label_description'] = label_descriptions\ntest.loc[:, 'label_score'] = label_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elimino las variables de tipo objeto.\n\ncolObject = list(train.dtypes[train.dtypes == \"object\"].index)\n\ntrain = train.drop(colObject,axis=1)\ntest = test.drop(colObject,axis=1)\n\nprint(f\"Variables eliminadas: {colObject}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convierto a las variables categoricas en tipo \"Category\" (conveniente para el algoritmo LGBM)\n\nnumeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'AdoptionSpeed', \n                'doc_sent_mag', 'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', \n                'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', \n                'bounding_confidence', 'vertex_x', 'vertex_y', 'label_score',\"name_length\",\"name_length\"] +\\\n               [col for col in train.columns if col.startswith('pic') or col.startswith('svd')]\ncat_cols = list(set(train.columns) - set(numeric_cols))\ntrain.loc[:, cat_cols] = train[cat_cols].astype('category')\ntest.loc[:, cat_cols] = test[cat_cols].astype('category')\nprint(train.shape)\nprint(test.shape)\n\n# Índice de las variables categoricas\ncatVal = train.dtypes\ncat_feature_names = catVal[catVal == \"category\"]\ncat_features_idx = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions -------------------","metadata":{}},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef get_kappa(model, X_train, X_valid, y_train, y_valid):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    y_pred_proba = model.predict_proba(X_valid)\n    return cohen_kappa_score(y_valid, y_pred, weights= 'quadratic'),y_pred_proba\n\ndef qwk_eval(preds, train_data):\n    y_true = train_data.get_label()\n    preds_rounded = np.round(preds)\n    return 'qwk', cohen_kappa_score(preds_rounded, y_true, weights='quadratic'), True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Optimización de los cortes. \n\n## Estuve tratando de resolver este tema con el OptimizedRounder (minimización de función objetivo para \n## encontrar los cortes óptimos a partir de una condición inicial) pero no tuve muy buenos resultados. \n## Luego probé cortar utilizando la distribución del conjunto de entrenamiento y los resultados mejoraron sustancialmente.\n\ndef get_thresholds_from_dist(y_true, y_pred):\n    \"\"\"Calculates thresholds for raw predictions\n    so as to follow the true distribution.\n    \"\"\"\n    idxs = np.cumsum(np.bincount(y_true))[:-1]\n    idxs = (idxs * y_pred.size / y_true.size).astype(int)\n    return np.sort(y_pred)[idxs]\n\ndef allocate_to_rate(y_pred, thresholds):\n    \"\"\"Allocates raw predictions to adoption rates.\"\"\"\n    rates = np.zeros(y_pred.size, dtype=int)\n    for i in range(4):\n        rates[y_pred >= thresholds[i]] = i + 1\n    return rates\n\ndef map_to_int(y_true, y_pred, preds):\n    thresholds = get_thresholds_from_dist(y_true, y_pred)\n    return allocate_to_rate(preds, thresholds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelado -----------------------------","metadata":{}},{"cell_type":"code","source":"## Separo el conjunto de entrenamiento en train y test_p\n\nX = train.drop(\"AdoptionSpeed\", axis=1)\ny = train.AdoptionSpeed\n\nX_train, X_test_p, y_train, y_test_p = train_test_split(X, y, \n                                                          test_size=0.15, \n                                                          random_state=0,\n                                                          stratify=y)\n\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"X_test_pre: {X_test_p.shape}\")\nprint(f\"y_train: {y_train.shape}\")\nprint(f\"y_test_pre: {y_test_p.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Definición del espacio de exploración de hiperparámetros.\n\nhyperparameter_combinations = []\nnum_trials = 100\n\nrandom.seed(10)\nnp.random.seed(10)\n\nfor _ in range(num_trials):\n    params = {\n        'objective': 'regression',\n        'boosting': 'gbdt',\n        'metric': 'rmse',\n        'num_leaves': np.random.randint(50, 90),\n        'max_depth': np.random.randint(3, 15),\n        'learning_rate': np.random.uniform(0.001, 0.5),\n        'bagging_fraction': np.random.uniform(0.50, 0.95),\n        'feature_fraction': np.random.uniform(0.40, 0.90),\n        'min_split_gain': np.random.uniform(0.008, 0.03),\n        'min_child_samples': np.random.randint(40, 200),\n        'min_child_weight': np.random.uniform(0.008, 0.03),\n        'lambda_l1': np.random.uniform(0.01, 0.06),\n        'lambda_l2': np.random.uniform(0.01, 0.06),          \n        'verbosity': -1,\n        'data_random_seed': 999,\n        'early_stopping_rounds': np.random.randint(100, 400),\n        #'device': 'gpu',    ## ------> El uso de GPU reduce el tiempo en aproximadamente 27%.\n        'deterministic': True\n    }\n    \n    hyperparameter_combinations.append(params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Entrenamiento utilizando:\n# - estrategia cross-validation,\n# - Optimización de hiperparámetros utilizando una estrategía Random Search sobre el espacio de busqueda de hiperparámetros definido arriba,\n# - Elección de los thresholds para discretizar el output de la regresión utilizando la distribución del conjunto de entrenamiento de cada fold.\n# - La métrica utilizada sobre validación es la misma que usa la competencia para evaluar nuestros resultados.\n\nbest_score = -np.inf\nbest_params = None\nbest_thresh = None\nmatrix_best_param = None\n\nstart = time.time()\n\nfor j,params in enumerate(hyperparameter_combinations):\n    \n    print(f\"Trial número {j+1}: \\n\")\n    print(\"Seleccionado hiperparámetros...\\n\")\n    print(f\"Los hiperparámetros seleccionados son {params}\\n\")\n    start_param = time.time()\n    \n    CANT_FOLDS_CV = 5\n    folds = StratifiedKFold(n_splits=CANT_FOLDS_CV,random_state=123,shuffle=True).split(X_train, y_train)\n    coef_all = np.zeros((CANT_FOLDS_CV,4))\n    qwk_all = np.zeros(CANT_FOLDS_CV)\n\n    for i,(train_indexFold, valid_indexFold) in enumerate(folds):\n        \n        start_fold = time.time()\n        print(f\"Entrenando el fold número {i+1}\\n\")\n        \n        # División de train y validaion para el fold actual\n        X_trainFold, X_valFold, y_trainFold, y_valFold = X.iloc[train_indexFold], X.iloc[valid_indexFold], y[train_indexFold], y[valid_indexFold]\n\n        d_train = lgb.Dataset(X_trainFold, \n                              label=y_trainFold,\n                              categorical_feature = cat_features_idx)\n\n        d_valid = lgb.Dataset(X_valFold, \n                              label=y_valFold,\n                              reference=d_train)\n\n        watchlist = [d_train, d_valid]\n        \n        # Entrenamiento del modelo\n        model = lgb.train(params,\n                          train_set=d_train,\n                          num_boost_round=10000,\n                          valid_sets=watchlist,\n                          feval = qwk_eval,\n                          #categorical_feature=cat_features_idx,\n                          callbacks=[lgb.early_stopping(stopping_rounds=params['early_stopping_rounds']),\n                                    lgb.log_evaluation(100)])\n        \n        # Perdicción sobre train y validation\n        y_trainFold_pred = model.predict(X_trainFold, num_iteration=model.best_iteration)\n        y_valFold_pred = model.predict(X_valFold, num_iteration=model.best_iteration)\n        \n        # Uso la predicción de train para obtener los thresholds\n        thresh = get_thresholds_from_dist(y_trainFold,y_trainFold_pred)\n        coef_all[i,:] = thresh\n        \n        # Usando los thresholds discretizo la predicción sobre validación\n        y_valFold_pred_adj = map_to_int(y_trainFold,y_trainFold_pred,y_valFold_pred)\n        \n        # Computo la métrica de la competencia sobre validación\n        qwk_fold = quadratic_weighted_kappa(y_valFold,y_valFold_pred_adj)\n        qwk_all[i] = qwk_fold\n        print(f\"La métrica QWK sobre validación del fold {i+1} es:{qwk_fold}\\n\")\n        \n        finish_fold = time.time()\n        #print(f'tiempo ejecución del fold {i+1} es: {finish_fold - start_fold} \\n')\n    \n    qwk_avg = qwk_all.mean()\n    avg_coef = coef_all.mean(axis=0)\n    \n    if qwk_avg > best_score:\n           \n        matrix_best_param = coef_all\n        best_thresh = avg_coef\n        best_score = qwk_avg\n        best_params = params\n    \n    finish_param = time.time()\n    #print('tiempo ejecución del primer set de parametros: {:.2f} \\n'.format(finish_param - start_param))\n    print(f\"El QWK promedio del fold {i+1} es: {qwk_avg}\\n\")\n    print(f\"El th promedio del fold {i+1} es: {avg_coef}\\n\")\n    print(f\"El QWK a superar es: {best_score}\\n\")\n    \nfinish = time.time()\nprint('tiempo ejecución: {:.2f} \\n'.format(finish - start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Matriz de cortes por cada fold:\")\nprint(matrix_best_param)\nprint(\"Sacando el promedio de los coeficientes...\")\navg_coef = best_thresh\nprint(f\"Los cortes promedio son: {avg_coef}\")\nprint(f\"Los hiperparámetros ganadores son: {best_params}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Entrenamiento sobre el total del conjunto de entrenamiento usando los parámetros que mejor resultaron en la etapa de validación.\n\nfinal_train_data = lgb.Dataset(X_train, \n                               label=y_train,\n                               categorical_feature = cat_features_idx)\n\nfinal_test_data = lgb.Dataset(X_test_p, \n                               label=y_test_p,\n                               reference=final_train_data)\n\nwatchlist = [final_train_data, final_test_data]\n\nmodel = lgb.train(best_params,\n                  train_set=final_train_data,\n                  num_boost_round=10000,\n                  valid_sets=watchlist,\n                  feval = qwk_eval,\n                  #categorical_feature=cat_features_idx,\n                  callbacks=[lgb.early_stopping(stopping_rounds=best_params['early_stopping_rounds']),\n                            lgb.log_evaluation(100)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicción sobre test (subconjunto del conjunto de entrenamiento que no se utilizó en la etapa de validación)\n\ntest_p_pred = model.predict(X_test_p, num_iteration=model.best_iteration)\ntestPre_pred_discr = allocate_to_rate(test_p_pred,avg_coef)\n\nprint(f\"El QWK en el conjunto de testeo es de: {quadratic_weighted_kappa(y_test_p,testPre_pred_discr)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicciones sobre test para la submission\n\ntest_pred = model.predict(test, num_iteration=model.best_iteration)\ntest_pred_discr = allocate_to_rate(test_pred,avg_coef)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels = pd.Series(test_pred_discr, \n                       name= \"AdoptionSpeed\",\n                       index=test.index)\n\ntest_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels.to_csv(\"submission.csv\")\n!head submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb.plot_importance(model, max_num_features = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}